{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning for Instruction Following \n",
    "\n",
    "This GPT is train to follow some instructions given an input, will work as a personal assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import GPT\n",
    "import GPTA\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\")\n",
    "data = GPTA.download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "# Format and visualize the input.\n",
    "model_input = GPTA.format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data) * 0.85)   \n",
    "test_portion = int(len(data) * 0.1)          \n",
    "val_portion = len(data) - train_portion - test_portion    \n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignore Token\n",
    "There is a difference between endoftext and a ignore token that is not going to be taking into account at the moment of the loss function, they are added after the first actual endoftext token. -> function input_preparation_txt()\n",
    "\n",
    "By default, PyTorch has the cross_entropy(..., ignore_index=-100) setting to ignore examples corresponding to the label -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT.create_tokenizer()\n",
    "device = GPT.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 1\n",
    "\n",
    "train_dataset = GPTA.InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=GPTA.input_preparation_txt,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "val_dataset = GPTA.InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=GPTA.input_preparation_txt,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = GPTA.InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=GPTA.input_preparation_txt,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "\n",
    "device = GPT.get_device()\n",
    "\n",
    "# Model cunfiguration\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # context \n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": True       # Query-key-value bias\n",
    "}\n",
    "gpt_a = GPT.GPTModel(GPT_CONFIG_124M)\n",
    "gpt_a.eval()\n",
    "\n",
    "# Load Weights from OpenAI\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "GPT.load_weights_into_gpt(gpt_a, params)\n",
    "gpt_a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n",
      "\n",
      "Note The passive mode may be in multiple places: the active sentence ends while the active sentence finishes.\n",
      "\n",
      "### Note the following instruction describes\n"
     ]
    }
   ],
   "source": [
    "# Behavior before the fine tuning\n",
    "input_text = GPTA.format_input(val_data[0])\n",
    "\n",
    "token_ids = GPT.text_generation(\n",
    "    model=gpt_a,\n",
    "    idx=GPT.text_to_token_ids(input_text, tokenizer).to(device),\n",
    "    num_token_generation=30,\n",
    "    top_k=30,\n",
    "    temperature=1.3,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "generated_text = GPT.token_ids_to_text(token_ids, tokenizer)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.252, Val loss 3.354\n",
      "Ep 1 (Step 000005): Train loss 1.912, Val loss 2.114\n",
      "Ep 1 (Step 000010): Train loss 1.221, Val loss 1.603\n",
      "Ep 1 (Step 000015): Train loss 1.467, Val loss 1.407\n",
      "Ep 1 (Step 000020): Train loss 1.430, Val loss 1.304\n",
      "Ep 1 (Step 000025): Train loss 0.870, Val loss 1.241\n",
      "Ep 1 (Step 000030): Train loss 1.153, Val loss 1.201\n",
      "Ep 1 (Step 000035): Train loss 1.321, Val loss 1.178\n",
      "Ep 1 (Step 000040): Train loss 0.882, Val loss 1.143\n",
      "Ep 1 (Step 000045): Train loss 1.198, Val loss 1.114\n",
      "Ep 1 (Step 000050): Train loss 0.792, Val loss 1.097\n",
      "Ep 1 (Step 000055): Train loss 1.078, Val loss 1.135\n",
      "Ep 1 (Step 000060): Train loss 1.036, Val loss 1.151\n",
      "Ep 1 (Step 000065): Train loss 0.828, Val loss 1.137\n",
      "Ep 1 (Step 000070): Train loss 1.257, Val loss 1.110\n",
      "Ep 1 (Step 000075): Train loss 0.702, Val loss 1.096\n",
      "Ep 1 (Step 000080): Train loss 1.182, Val loss 1.098\n",
      "Ep 1 (Step 000085): Train loss 1.221, Val loss 1.099\n",
      "Ep 1 (Step 000090): Train loss 0.904, Val loss 1.092\n",
      "Ep 1 (Step 000095): Train loss 0.839, Val loss 1.075\n",
      "Ep 1 (Step 000100): Train loss 0.790, Val loss 1.089\n",
      "Ep 1 (Step 000105): Train loss 1.011, Val loss 1.082\n",
      "Ep 1 (Step 000110): Train loss 0.879, Val loss 1.088\n",
      "Ep 1 (Step 000115): Train loss 0.930, Val loss 1.101\n",
      "Ep 1 (Step 000120): Train loss 0.697, Val loss 1.108\n",
      "Ep 1 (Step 000125): Train loss 0.980, Val loss 1.112\n",
      "Ep 1 (Step 000130): Train loss 0.540, Val loss 1.098\n",
      "Ep 1 (Step 000135): Train loss 0.850, Val loss 1.097\n",
      "Ep 1 (Step 000140): Train loss 0.928, Val loss 1.095\n",
      "Ep 1 (Step 000145): Train loss 0.725, Val loss 1.090\n",
      "Ep 1 (Step 000150): Train loss 0.790, Val loss 1.085\n",
      "Ep 1 (Step 000155): Train loss 0.918, Val loss 1.079\n",
      "Ep 1 (Step 000160): Train loss 0.745, Val loss 1.058\n",
      "Ep 1 (Step 000165): Train loss 0.600, Val loss 1.058\n",
      "Ep 1 (Step 000170): Train loss 0.647, Val loss 1.050\n",
      "Ep 1 (Step 000175): Train loss 0.969, Val loss 1.048\n",
      "Ep 1 (Step 000180): Train loss 0.675, Val loss 1.045\n",
      "Ep 1 (Step 000185): Train loss 0.933, Val loss 1.055\n",
      "Ep 1 (Step 000190): Train loss 0.557, Val loss 1.049\n",
      "Ep 1 (Step 000195): Train loss 1.038, Val loss 1.042\n",
      "Ep 1 (Step 000200): Train loss 0.803, Val loss 1.038\n",
      "Ep 1 (Step 000205): Train loss 1.027, Val loss 1.034\n",
      "Ep 1 (Step 000210): Train loss 1.190, Val loss 1.027\n",
      "Ep 1 (Step 000215): Train loss 0.744, Val loss 1.036\n",
      "Ep 1 (Step 000220): Train loss 0.781, Val loss 1.032\n",
      "Ep 1 (Step 000225): Train loss 0.693, Val loss 1.031\n",
      "Ep 1 (Step 000230): Train loss 0.631, Val loss 1.033\n",
      "Ep 1 (Step 000235): Train loss 0.858, Val loss 1.033\n",
      "Ep 1 (Step 000240): Train loss 1.013, Val loss 1.035\n",
      "Ep 1 (Step 000245): Train loss 0.899, Val loss 1.026\n",
      "Ep 1 (Step 000250): Train loss 0.711, Val loss 1.017\n",
      "Ep 1 (Step 000255): Train loss 0.925, Val loss 1.014\n",
      "Ep 1 (Step 000260): Train loss 0.856, Val loss 1.012\n",
      "Ep 1 (Step 000265): Train loss 0.417, Val loss 1.027\n",
      "Ep 1 (Step 000270): Train loss 1.021, Val loss 1.028\n",
      "Ep 1 (Step 000275): Train loss 0.691, Val loss 1.027\n",
      "Ep 1 (Step 000280): Train loss 0.745, Val loss 1.025\n",
      "Ep 1 (Step 000285): Train loss 0.892, Val loss 1.024\n",
      "Ep 1 (Step 000290): Train loss 0.766, Val loss 1.000\n",
      "Ep 1 (Step 000295): Train loss 0.539, Val loss 0.991\n",
      "Ep 1 (Step 000300): Train loss 0.786, Val loss 0.961\n",
      "Ep 1 (Step 000305): Train loss 0.705, Val loss 0.942\n",
      "Ep 1 (Step 000310): Train loss 0.746, Val loss 0.931\n",
      "Ep 1 (Step 000315): Train loss 0.759, Val loss 0.925\n",
      "Ep 1 (Step 000320): Train loss 0.866, Val loss 0.912\n",
      "Ep 1 (Step 000325): Train loss 0.843, Val loss 0.906\n",
      "Ep 1 (Step 000330): Train loss 0.630, Val loss 0.907\n",
      "Ep 1 (Step 000335): Train loss 0.694, Val loss 0.895\n",
      "Ep 1 (Step 000340): Train loss 0.847, Val loss 0.882\n",
      "Ep 1 (Step 000345): Train loss 0.784, Val loss 0.885\n",
      "Ep 1 (Step 000350): Train loss 0.669, Val loss 0.897\n",
      "Ep 1 (Step 000355): Train loss 0.695, Val loss 0.902\n",
      "Ep 1 (Step 000360): Train loss 0.712, Val loss 0.897\n",
      "Ep 1 (Step 000365): Train loss 0.698, Val loss 0.899\n",
      "Ep 1 (Step 000370): Train loss 0.706, Val loss 0.900\n",
      "Ep 1 (Step 000375): Train loss 0.754, Val loss 0.896\n",
      "Ep 1 (Step 000380): Train loss 0.576, Val loss 0.894\n",
      "Ep 1 (Step 000385): Train loss 0.749, Val loss 0.892\n",
      "Ep 1 (Step 000390): Train loss 0.740, Val loss 0.881\n",
      "Ep 1 (Step 000395): Train loss 0.847, Val loss 0.873\n",
      "Ep 1 (Step 000400): Train loss 0.651, Val loss 0.878\n",
      "Ep 1 (Step 000405): Train loss 0.605, Val loss 0.890\n",
      "Ep 1 (Step 000410): Train loss 0.703, Val loss 0.892\n",
      "Ep 1 (Step 000415): Train loss 0.776, Val loss 0.898\n",
      "Ep 1 (Step 000420): Train loss 0.695, Val loss 0.901\n",
      "Ep 1 (Step 000425): Train loss 0.682, Val loss 0.898\n",
      "Ep 1 (Step 000430): Train loss 0.813, Val loss 0.894\n",
      "Ep 1 (Step 000435): Train loss 0.541, Val loss 0.891\n",
      "Ep 1 (Step 000440): Train loss 0.500, Val loss 0.878\n",
      "Ep 1 (Step 000445): Train loss 0.854, Val loss 0.867\n",
      "Ep 1 (Step 000450): Train loss 0.997, Val loss 0.861\n",
      "Ep 1 (Step 000455): Train loss 0.852, Val loss 0.853\n",
      "Ep 1 (Step 000460): Train loss 0.904, Val loss 0.845\n",
      "Ep 1 (Step 000465): Train loss 0.659, Val loss 0.847\n",
      "Ep 1 (Step 000470): Train loss 0.687, Val loss 0.847\n",
      "Ep 1 (Step 000475): Train loss 1.016, Val loss 0.839\n",
      "Ep 1 (Step 000480): Train loss 0.649, Val loss 0.828\n",
      "Ep 1 (Step 000485): Train loss 0.877, Val loss 0.830\n",
      "Ep 1 (Step 000490): Train loss 0.927, Val loss 0.822\n",
      "Ep 1 (Step 000495): Train loss 0.794, Val loss 0.817\n",
      "Ep 1 (Step 000500): Train loss 0.606, Val loss 0.823\n",
      "Ep 1 (Step 000505): Train loss 0.541, Val loss 0.829\n",
      "Ep 1 (Step 000510): Train loss 1.048, Val loss 0.830\n",
      "Ep 1 (Step 000515): Train loss 0.556, Val loss 0.830\n",
      "Ep 1 (Step 000520): Train loss 0.748, Val loss 0.831\n",
      "Ep 1 (Step 000525): Train loss 0.790, Val loss 0.836\n",
      "Ep 1 (Step 000530): Train loss 0.452, Val loss 0.844\n",
      "Ep 1 (Step 000535): Train loss 0.511, Val loss 0.853\n",
      "Ep 1 (Step 000540): Train loss 0.825, Val loss 0.847\n",
      "Ep 1 (Step 000545): Train loss 0.794, Val loss 0.839\n",
      "Ep 1 (Step 000550): Train loss 0.371, Val loss 0.829\n",
      "Ep 1 (Step 000555): Train loss 0.717, Val loss 0.828\n",
      "Ep 1 (Step 000560): Train loss 0.738, Val loss 0.826\n",
      "Ep 1 (Step 000565): Train loss 0.497, Val loss 0.824\n",
      "Ep 1 (Step 000570): Train loss 0.494, Val loss 0.832\n",
      "Ep 1 (Step 000575): Train loss 0.679, Val loss 0.834\n",
      "Ep 1 (Step 000580): Train loss 0.534, Val loss 0.837\n",
      "Ep 1 (Step 000585): Train loss 0.585, Val loss 0.833\n",
      "Ep 1 (Step 000590): Train loss 0.733, Val loss 0.829\n",
      "Ep 1 (Step 000595): Train loss 0.457, Val loss 0.832\n",
      "Ep 1 (Step 000600): Train loss 0.572, Val loss 0.829\n",
      "Ep 1 (Step 000605): Train loss 0.711, Val loss 0.820\n",
      "Ep 1 (Step 000610): Train loss 0.512, Val loss 0.817\n",
      "Ep 1 (Step 000615): Train loss 0.430, Val loss 0.818\n",
      "Ep 1 (Step 000620): Train loss 0.833, Val loss 0.824\n",
      "Ep 1 (Step 000625): Train loss 0.512, Val loss 0.833\n",
      "Ep 1 (Step 000630): Train loss 0.426, Val loss 0.834\n",
      "Ep 1 (Step 000635): Train loss 0.494, Val loss 0.844\n",
      "Ep 1 (Step 000640): Train loss 0.517, Val loss 0.847\n",
      "Ep 1 (Step 000645): Train loss 0.640, Val loss 0.849\n",
      "Ep 1 (Step 000650): Train loss 0.522, Val loss 0.846\n",
      "Ep 1 (Step 000655): Train loss 0.517, Val loss 0.842\n",
      "Ep 1 (Step 000660): Train loss 0.431, Val loss 0.847\n",
      "Ep 1 (Step 000665): Train loss 0.442, Val loss 0.849\n",
      "Ep 1 (Step 000670): Train loss 0.447, Val loss 0.854\n",
      "Ep 1 (Step 000675): Train loss 0.398, Val loss 0.856\n",
      "Ep 1 (Step 000680): Train loss 0.771, Val loss 0.858\n",
      "Ep 1 (Step 000685): Train loss 0.502, Val loss 0.859\n",
      "Ep 1 (Step 000690): Train loss 0.723, Val loss 0.849\n",
      "Ep 1 (Step 000695): Train loss 0.544, Val loss 0.836\n",
      "Ep 1 (Step 000700): Train loss 0.566, Val loss 0.830\n",
      "Ep 1 (Step 000705): Train loss 0.570, Val loss 0.829\n",
      "Ep 1 (Step 000710): Train loss 0.320, Val loss 0.822\n",
      "Ep 1 (Step 000715): Train loss 0.500, Val loss 0.813\n",
      "Ep 1 (Step 000720): Train loss 0.653, Val loss 0.810\n",
      "Ep 1 (Step 000725): Train loss 0.505, Val loss 0.817\n",
      "Ep 1 (Step 000730): Train loss 0.544, Val loss 0.810\n",
      "Ep 1 (Step 000735): Train loss 0.593, Val loss 0.805\n",
      "Ep 1 (Step 000740): Train loss 0.396, Val loss 0.806\n",
      "Ep 1 (Step 000745): Train loss 0.564, Val loss 0.806\n",
      "Ep 1 (Step 000750): Train loss 0.525, Val loss 0.801\n",
      "Ep 1 (Step 000755): Train loss 0.672, Val loss 0.798\n",
      "Ep 1 (Step 000760): Train loss 0.654, Val loss 0.795\n",
      "Ep 1 (Step 000765): Train loss 0.660, Val loss 0.799\n",
      "Ep 1 (Step 000770): Train loss 0.818, Val loss 0.805\n",
      "Ep 1 (Step 000775): Train loss 0.589, Val loss 0.801\n",
      "Ep 1 (Step 000780): Train loss 0.425, Val loss 0.793\n",
      "Ep 1 (Step 000785): Train loss 0.603, Val loss 0.789\n",
      "Ep 1 (Step 000790): Train loss 0.415, Val loss 0.788\n",
      "Ep 1 (Step 000795): Train loss 0.423, Val loss 0.785\n",
      "Ep 1 (Step 000800): Train loss 0.637, Val loss 0.785\n",
      "Ep 1 (Step 000805): Train loss 0.579, Val loss 0.788\n",
      "Ep 1 (Step 000810): Train loss 0.747, Val loss 0.792\n",
      "Ep 1 (Step 000815): Train loss 0.448, Val loss 0.791\n",
      "Ep 1 (Step 000820): Train loss 0.620, Val loss 0.796\n",
      "Ep 1 (Step 000825): Train loss 0.589, Val loss 0.806\n",
      "Ep 1 (Step 000830): Train loss 0.568, Val loss 0.818\n",
      "Ep 1 (Step 000835): Train loss 0.879, Val loss 0.817\n",
      "Ep 1 (Step 000840): Train loss 0.602, Val loss 0.811\n",
      "Ep 1 (Step 000845): Train loss 0.548, Val loss 0.804\n",
      "Ep 1 (Step 000850): Train loss 0.564, Val loss 0.801\n",
      "Ep 1 (Step 000855): Train loss 0.658, Val loss 0.804\n",
      "Ep 1 (Step 000860): Train loss 0.478, Val loss 0.811\n",
      "Ep 1 (Step 000865): Train loss 0.511, Val loss 0.794\n",
      "Ep 1 (Step 000870): Train loss 0.644, Val loss 0.772\n",
      "Ep 1 (Step 000875): Train loss 0.497, Val loss 0.760\n",
      "Ep 1 (Step 000880): Train loss 0.388, Val loss 0.753\n",
      "Ep 1 (Step 000885): Train loss 0.391, Val loss 0.751\n",
      "Ep 1 (Step 000890): Train loss 0.427, Val loss 0.740\n",
      "Ep 1 (Step 000895): Train loss 0.597, Val loss 0.739\n",
      "Ep 1 (Step 000900): Train loss 0.344, Val loss 0.737\n",
      "Ep 1 (Step 000905): Train loss 0.526, Val loss 0.749\n",
      "Ep 1 (Step 000910): Train loss 0.538, Val loss 0.759\n",
      "Ep 1 (Step 000915): Train loss 0.360, Val loss 0.785\n",
      "Ep 1 (Step 000920): Train loss 0.656, Val loss 0.811\n",
      "Ep 1 (Step 000925): Train loss 0.860, Val loss 0.808\n",
      "Ep 1 (Step 000930): Train loss 0.464, Val loss 0.787\n",
      "Text Generation Sample\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: \"He prepares the meal every day.\"<|endoftext|>What is the chemical formula for ethanol?  The chemical formula for ethanol is ethanolH4.<|endoftext|>A lot of people\n",
      "Ep 2 (Step 000935): Train loss 0.476, Val loss 0.773\n",
      "Ep 2 (Step 000940): Train loss 0.753, Val loss 0.774\n",
      "Ep 2 (Step 000945): Train loss 0.545, Val loss 0.773\n",
      "Ep 2 (Step 000950): Train loss 0.490, Val loss 0.774\n",
      "Ep 2 (Step 000955): Train loss 0.732, Val loss 0.775\n",
      "Ep 2 (Step 000960): Train loss 0.610, Val loss 0.785\n",
      "Ep 2 (Step 000965): Train loss 0.501, Val loss 0.793\n",
      "Ep 2 (Step 000970): Train loss 0.384, Val loss 0.788\n",
      "Ep 2 (Step 000975): Train loss 0.342, Val loss 0.773\n",
      "Ep 2 (Step 000980): Train loss 0.553, Val loss 0.763\n",
      "Ep 2 (Step 000985): Train loss 0.519, Val loss 0.759\n",
      "Ep 2 (Step 000990): Train loss 0.619, Val loss 0.759\n",
      "Ep 2 (Step 000995): Train loss 0.441, Val loss 0.758\n",
      "Ep 2 (Step 001000): Train loss 0.485, Val loss 0.758\n",
      "Ep 2 (Step 001005): Train loss 0.651, Val loss 0.760\n",
      "Ep 2 (Step 001010): Train loss 0.595, Val loss 0.758\n",
      "Ep 2 (Step 001015): Train loss 0.490, Val loss 0.760\n",
      "Ep 2 (Step 001020): Train loss 0.463, Val loss 0.758\n",
      "Ep 2 (Step 001025): Train loss 0.437, Val loss 0.763\n",
      "Ep 2 (Step 001030): Train loss 0.478, Val loss 0.769\n",
      "Ep 2 (Step 001035): Train loss 0.449, Val loss 0.771\n",
      "Ep 2 (Step 001040): Train loss 0.398, Val loss 0.768\n",
      "Ep 2 (Step 001045): Train loss 0.457, Val loss 0.766\n",
      "Ep 2 (Step 001050): Train loss 0.467, Val loss 0.763\n",
      "Ep 2 (Step 001055): Train loss 0.476, Val loss 0.765\n",
      "Ep 2 (Step 001060): Train loss 0.479, Val loss 0.774\n",
      "Ep 2 (Step 001065): Train loss 0.750, Val loss 0.784\n",
      "Ep 2 (Step 001070): Train loss 0.564, Val loss 0.794\n",
      "Ep 2 (Step 001075): Train loss 0.450, Val loss 0.796\n",
      "Ep 2 (Step 001080): Train loss 0.463, Val loss 0.773\n",
      "Ep 2 (Step 001085): Train loss 0.372, Val loss 0.754\n",
      "Ep 2 (Step 001090): Train loss 0.454, Val loss 0.745\n",
      "Ep 2 (Step 001095): Train loss 0.549, Val loss 0.741\n",
      "Ep 2 (Step 001100): Train loss 0.602, Val loss 0.742\n",
      "Ep 2 (Step 001105): Train loss 0.421, Val loss 0.737\n",
      "Ep 2 (Step 001110): Train loss 0.568, Val loss 0.723\n",
      "Ep 2 (Step 001115): Train loss 0.310, Val loss 0.719\n",
      "Ep 2 (Step 001120): Train loss 0.329, Val loss 0.719\n",
      "Ep 2 (Step 001125): Train loss 0.388, Val loss 0.711\n",
      "Ep 2 (Step 001130): Train loss 0.469, Val loss 0.707\n",
      "Ep 2 (Step 001135): Train loss 0.334, Val loss 0.705\n",
      "Ep 2 (Step 001140): Train loss 0.547, Val loss 0.709\n",
      "Ep 2 (Step 001145): Train loss 0.367, Val loss 0.712\n",
      "Ep 2 (Step 001150): Train loss 0.282, Val loss 0.711\n",
      "Ep 2 (Step 001155): Train loss 0.423, Val loss 0.712\n",
      "Ep 2 (Step 001160): Train loss 0.469, Val loss 0.715\n",
      "Ep 2 (Step 001165): Train loss 0.362, Val loss 0.710\n",
      "Ep 2 (Step 001170): Train loss 0.412, Val loss 0.710\n",
      "Ep 2 (Step 001175): Train loss 0.655, Val loss 0.713\n",
      "Ep 2 (Step 001180): Train loss 0.471, Val loss 0.722\n",
      "Ep 2 (Step 001185): Train loss 0.653, Val loss 0.726\n",
      "Ep 2 (Step 001190): Train loss 0.410, Val loss 0.726\n",
      "Ep 2 (Step 001195): Train loss 0.715, Val loss 0.729\n",
      "Ep 2 (Step 001200): Train loss 0.381, Val loss 0.734\n",
      "Ep 2 (Step 001205): Train loss 0.413, Val loss 0.742\n",
      "Ep 2 (Step 001210): Train loss 0.477, Val loss 0.756\n",
      "Ep 2 (Step 001215): Train loss 0.451, Val loss 0.756\n",
      "Ep 2 (Step 001220): Train loss 0.616, Val loss 0.752\n",
      "Ep 2 (Step 001225): Train loss 0.450, Val loss 0.754\n",
      "Ep 2 (Step 001230): Train loss 0.318, Val loss 0.758\n",
      "Ep 2 (Step 001235): Train loss 0.380, Val loss 0.757\n",
      "Ep 2 (Step 001240): Train loss 0.682, Val loss 0.751\n",
      "Ep 2 (Step 001245): Train loss 0.577, Val loss 0.747\n",
      "Ep 2 (Step 001250): Train loss 0.407, Val loss 0.733\n",
      "Ep 2 (Step 001255): Train loss 0.340, Val loss 0.731\n",
      "Ep 2 (Step 001260): Train loss 0.426, Val loss 0.731\n",
      "Ep 2 (Step 001265): Train loss 0.588, Val loss 0.732\n",
      "Ep 2 (Step 001270): Train loss 0.376, Val loss 0.739\n",
      "Ep 2 (Step 001275): Train loss 0.568, Val loss 0.752\n",
      "Ep 2 (Step 001280): Train loss 0.388, Val loss 0.758\n",
      "Ep 2 (Step 001285): Train loss 0.333, Val loss 0.758\n",
      "Ep 2 (Step 001290): Train loss 0.560, Val loss 0.762\n",
      "Ep 2 (Step 001295): Train loss 0.393, Val loss 0.769\n",
      "Ep 2 (Step 001300): Train loss 0.635, Val loss 0.791\n",
      "Ep 2 (Step 001305): Train loss 0.522, Val loss 0.799\n",
      "Ep 2 (Step 001310): Train loss 0.396, Val loss 0.800\n",
      "Ep 2 (Step 001315): Train loss 0.314, Val loss 0.801\n",
      "Ep 2 (Step 001320): Train loss 0.474, Val loss 0.796\n",
      "Ep 2 (Step 001325): Train loss 0.533, Val loss 0.783\n",
      "Ep 2 (Step 001330): Train loss 0.489, Val loss 0.776\n",
      "Ep 2 (Step 001335): Train loss 0.600, Val loss 0.773\n",
      "Ep 2 (Step 001340): Train loss 0.529, Val loss 0.774\n",
      "Ep 2 (Step 001345): Train loss 0.350, Val loss 0.778\n",
      "Ep 2 (Step 001350): Train loss 0.351, Val loss 0.786\n",
      "Ep 2 (Step 001355): Train loss 0.432, Val loss 0.793\n",
      "Ep 2 (Step 001360): Train loss 0.516, Val loss 0.798\n",
      "Ep 2 (Step 001365): Train loss 0.578, Val loss 0.804\n",
      "Ep 2 (Step 001370): Train loss 0.469, Val loss 0.810\n",
      "Ep 2 (Step 001375): Train loss 0.368, Val loss 0.808\n",
      "Ep 2 (Step 001380): Train loss 0.483, Val loss 0.798\n",
      "Ep 2 (Step 001385): Train loss 0.347, Val loss 0.774\n",
      "Ep 2 (Step 001390): Train loss 0.588, Val loss 0.753\n",
      "Ep 2 (Step 001395): Train loss 0.486, Val loss 0.733\n",
      "Ep 2 (Step 001400): Train loss 0.493, Val loss 0.730\n",
      "Ep 2 (Step 001405): Train loss 0.365, Val loss 0.721\n",
      "Ep 2 (Step 001410): Train loss 0.449, Val loss 0.719\n",
      "Ep 2 (Step 001415): Train loss 0.331, Val loss 0.722\n",
      "Ep 2 (Step 001420): Train loss 0.538, Val loss 0.723\n",
      "Ep 2 (Step 001425): Train loss 0.357, Val loss 0.728\n",
      "Ep 2 (Step 001430): Train loss 0.465, Val loss 0.732\n",
      "Ep 2 (Step 001435): Train loss 0.477, Val loss 0.742\n",
      "Ep 2 (Step 001440): Train loss 0.385, Val loss 0.757\n",
      "Ep 2 (Step 001445): Train loss 0.477, Val loss 0.769\n",
      "Ep 2 (Step 001450): Train loss 0.293, Val loss 0.782\n",
      "Ep 2 (Step 001455): Train loss 0.480, Val loss 0.792\n",
      "Ep 2 (Step 001460): Train loss 0.524, Val loss 0.790\n",
      "Ep 2 (Step 001465): Train loss 0.414, Val loss 0.785\n",
      "Ep 2 (Step 001470): Train loss 0.438, Val loss 0.770\n",
      "Ep 2 (Step 001475): Train loss 0.316, Val loss 0.756\n",
      "Ep 2 (Step 001480): Train loss 0.360, Val loss 0.747\n",
      "Ep 2 (Step 001485): Train loss 0.240, Val loss 0.744\n",
      "Ep 2 (Step 001490): Train loss 0.322, Val loss 0.743\n",
      "Ep 2 (Step 001495): Train loss 0.429, Val loss 0.745\n",
      "Ep 2 (Step 001500): Train loss 0.427, Val loss 0.741\n",
      "Ep 2 (Step 001505): Train loss 0.395, Val loss 0.732\n",
      "Ep 2 (Step 001510): Train loss 0.481, Val loss 0.729\n",
      "Ep 2 (Step 001515): Train loss 0.530, Val loss 0.723\n",
      "Ep 2 (Step 001520): Train loss 0.418, Val loss 0.724\n",
      "Ep 2 (Step 001525): Train loss 0.482, Val loss 0.730\n",
      "Ep 2 (Step 001530): Train loss 0.243, Val loss 0.734\n",
      "Ep 2 (Step 001535): Train loss 0.427, Val loss 0.740\n",
      "Ep 2 (Step 001540): Train loss 0.698, Val loss 0.741\n",
      "Ep 2 (Step 001545): Train loss 0.399, Val loss 0.740\n",
      "Ep 2 (Step 001550): Train loss 0.556, Val loss 0.742\n",
      "Ep 2 (Step 001555): Train loss 0.335, Val loss 0.754\n",
      "Ep 2 (Step 001560): Train loss 0.467, Val loss 0.762\n",
      "Ep 2 (Step 001565): Train loss 0.428, Val loss 0.764\n",
      "Ep 2 (Step 001570): Train loss 0.493, Val loss 0.757\n",
      "Ep 2 (Step 001575): Train loss 0.524, Val loss 0.744\n",
      "Ep 2 (Step 001580): Train loss 0.390, Val loss 0.734\n",
      "Ep 2 (Step 001585): Train loss 0.211, Val loss 0.729\n",
      "Ep 2 (Step 001590): Train loss 0.427, Val loss 0.726\n",
      "Ep 2 (Step 001595): Train loss 0.384, Val loss 0.726\n",
      "Ep 2 (Step 001600): Train loss 0.293, Val loss 0.730\n",
      "Ep 2 (Step 001605): Train loss 0.455, Val loss 0.733\n",
      "Ep 2 (Step 001610): Train loss 0.426, Val loss 0.738\n",
      "Ep 2 (Step 001615): Train loss 0.353, Val loss 0.745\n",
      "Ep 2 (Step 001620): Train loss 0.453, Val loss 0.748\n",
      "Ep 2 (Step 001625): Train loss 0.498, Val loss 0.742\n",
      "Ep 2 (Step 001630): Train loss 0.352, Val loss 0.732\n",
      "Ep 2 (Step 001635): Train loss 0.362, Val loss 0.729\n",
      "Ep 2 (Step 001640): Train loss 0.472, Val loss 0.730\n",
      "Ep 2 (Step 001645): Train loss 0.521, Val loss 0.732\n",
      "Ep 2 (Step 001650): Train loss 0.355, Val loss 0.736\n",
      "Ep 2 (Step 001655): Train loss 0.425, Val loss 0.730\n",
      "Ep 2 (Step 001660): Train loss 0.345, Val loss 0.726\n",
      "Ep 2 (Step 001665): Train loss 0.430, Val loss 0.714\n",
      "Ep 2 (Step 001670): Train loss 0.333, Val loss 0.716\n",
      "Ep 2 (Step 001675): Train loss 0.291, Val loss 0.719\n",
      "Ep 2 (Step 001680): Train loss 0.470, Val loss 0.693\n",
      "Ep 2 (Step 001685): Train loss 0.420, Val loss 0.683\n",
      "Ep 2 (Step 001690): Train loss 0.235, Val loss 0.683\n",
      "Ep 2 (Step 001695): Train loss 0.494, Val loss 0.686\n",
      "Ep 2 (Step 001700): Train loss 0.338, Val loss 0.688\n",
      "Ep 2 (Step 001705): Train loss 0.485, Val loss 0.678\n",
      "Ep 2 (Step 001710): Train loss 0.361, Val loss 0.675\n",
      "Ep 2 (Step 001715): Train loss 0.327, Val loss 0.674\n",
      "Ep 2 (Step 001720): Train loss 0.388, Val loss 0.682\n",
      "Ep 2 (Step 001725): Train loss 0.656, Val loss 0.673\n",
      "Ep 2 (Step 001730): Train loss 0.354, Val loss 0.662\n",
      "Ep 2 (Step 001735): Train loss 0.388, Val loss 0.664\n",
      "Ep 2 (Step 001740): Train loss 0.461, Val loss 0.669\n",
      "Ep 2 (Step 001745): Train loss 0.351, Val loss 0.674\n",
      "Ep 2 (Step 001750): Train loss 0.498, Val loss 0.679\n",
      "Ep 2 (Step 001755): Train loss 0.406, Val loss 0.679\n",
      "Ep 2 (Step 001760): Train loss 0.424, Val loss 0.679\n",
      "Ep 2 (Step 001765): Train loss 0.285, Val loss 0.678\n",
      "Ep 2 (Step 001770): Train loss 0.263, Val loss 0.674\n",
      "Ep 2 (Step 001775): Train loss 0.291, Val loss 0.669\n",
      "Ep 2 (Step 001780): Train loss 0.348, Val loss 0.668\n",
      "Ep 2 (Step 001785): Train loss 0.381, Val loss 0.670\n",
      "Ep 2 (Step 001790): Train loss 0.389, Val loss 0.672\n",
      "Ep 2 (Step 001795): Train loss 0.361, Val loss 0.674\n",
      "Ep 2 (Step 001800): Train loss 0.414, Val loss 0.678\n",
      "Ep 2 (Step 001805): Train loss 0.321, Val loss 0.685\n",
      "Ep 2 (Step 001810): Train loss 0.446, Val loss 0.695\n",
      "Ep 2 (Step 001815): Train loss 0.403, Val loss 0.708\n",
      "Ep 2 (Step 001820): Train loss 0.409, Val loss 0.718\n",
      "Ep 2 (Step 001825): Train loss 0.352, Val loss 0.721\n",
      "Ep 2 (Step 001830): Train loss 0.379, Val loss 0.726\n",
      "Ep 2 (Step 001835): Train loss 0.343, Val loss 0.733\n",
      "Ep 2 (Step 001840): Train loss 0.352, Val loss 0.735\n",
      "Ep 2 (Step 001845): Train loss 0.376, Val loss 0.734\n",
      "Ep 2 (Step 001850): Train loss 0.302, Val loss 0.725\n",
      "Ep 2 (Step 001855): Train loss 0.292, Val loss 0.721\n",
      "Ep 2 (Step 001860): Train loss 0.329, Val loss 0.718\n",
      "Ep 2 (Step 001865): Train loss 0.250, Val loss 0.719\n",
      "Text Generation Sample\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Input: The chef's meal breaks down into quick meals each evening.  ### Response: The meal, produced each evening, can be quickly cooked.<|endoftext|>The cityscape\n",
      "Ep 3 (Step 001870): Train loss 0.377, Val loss 0.722\n",
      "Ep 3 (Step 001875): Train loss 0.380, Val loss 0.727\n",
      "Ep 3 (Step 001880): Train loss 0.380, Val loss 0.734\n",
      "Ep 3 (Step 001885): Train loss 0.211, Val loss 0.736\n",
      "Ep 3 (Step 001890): Train loss 0.409, Val loss 0.737\n",
      "Ep 3 (Step 001895): Train loss 0.280, Val loss 0.730\n",
      "Ep 3 (Step 001900): Train loss 0.270, Val loss 0.722\n",
      "Ep 3 (Step 001905): Train loss 0.240, Val loss 0.722\n",
      "Ep 3 (Step 001910): Train loss 0.310, Val loss 0.728\n",
      "Ep 3 (Step 001915): Train loss 0.360, Val loss 0.733\n",
      "Ep 3 (Step 001920): Train loss 0.262, Val loss 0.725\n",
      "Ep 3 (Step 001925): Train loss 0.334, Val loss 0.726\n",
      "Ep 3 (Step 001930): Train loss 0.457, Val loss 0.735\n",
      "Ep 3 (Step 001935): Train loss 0.333, Val loss 0.741\n",
      "Ep 3 (Step 001940): Train loss 0.441, Val loss 0.745\n",
      "Ep 3 (Step 001945): Train loss 0.263, Val loss 0.751\n",
      "Ep 3 (Step 001950): Train loss 0.415, Val loss 0.767\n",
      "Ep 3 (Step 001955): Train loss 0.244, Val loss 0.782\n",
      "Ep 3 (Step 001960): Train loss 0.395, Val loss 0.790\n",
      "Ep 3 (Step 001965): Train loss 0.457, Val loss 0.776\n",
      "Ep 3 (Step 001970): Train loss 0.297, Val loss 0.752\n",
      "Ep 3 (Step 001975): Train loss 0.445, Val loss 0.734\n",
      "Ep 3 (Step 001980): Train loss 0.256, Val loss 0.730\n",
      "Ep 3 (Step 001985): Train loss 0.272, Val loss 0.719\n",
      "Ep 3 (Step 001990): Train loss 0.320, Val loss 0.715\n",
      "Ep 3 (Step 001995): Train loss 0.247, Val loss 0.712\n",
      "Ep 3 (Step 002000): Train loss 0.285, Val loss 0.714\n",
      "Ep 3 (Step 002005): Train loss 0.463, Val loss 0.711\n",
      "Ep 3 (Step 002010): Train loss 0.347, Val loss 0.709\n",
      "Ep 3 (Step 002015): Train loss 0.326, Val loss 0.716\n",
      "Ep 3 (Step 002020): Train loss 0.269, Val loss 0.725\n",
      "Ep 3 (Step 002025): Train loss 0.365, Val loss 0.736\n",
      "Ep 3 (Step 002030): Train loss 0.259, Val loss 0.740\n",
      "Ep 3 (Step 002035): Train loss 0.513, Val loss 0.735\n",
      "Ep 3 (Step 002040): Train loss 0.322, Val loss 0.735\n",
      "Ep 3 (Step 002045): Train loss 0.311, Val loss 0.733\n",
      "Ep 3 (Step 002050): Train loss 0.323, Val loss 0.725\n",
      "Ep 3 (Step 002055): Train loss 0.313, Val loss 0.718\n",
      "Ep 3 (Step 002060): Train loss 0.257, Val loss 0.721\n",
      "Ep 3 (Step 002065): Train loss 0.386, Val loss 0.719\n",
      "Ep 3 (Step 002070): Train loss 0.331, Val loss 0.710\n",
      "Ep 3 (Step 002075): Train loss 0.447, Val loss 0.700\n",
      "Ep 3 (Step 002080): Train loss 0.329, Val loss 0.694\n",
      "Ep 3 (Step 002085): Train loss 0.376, Val loss 0.691\n",
      "Ep 3 (Step 002090): Train loss 0.289, Val loss 0.690\n",
      "Ep 3 (Step 002095): Train loss 0.399, Val loss 0.691\n",
      "Ep 3 (Step 002100): Train loss 0.447, Val loss 0.696\n",
      "Ep 3 (Step 002105): Train loss 0.408, Val loss 0.700\n",
      "Ep 3 (Step 002110): Train loss 0.335, Val loss 0.702\n",
      "Ep 3 (Step 002115): Train loss 0.314, Val loss 0.717\n",
      "Ep 3 (Step 002120): Train loss 0.348, Val loss 0.739\n",
      "Ep 3 (Step 002125): Train loss 0.368, Val loss 0.724\n",
      "Ep 3 (Step 002130): Train loss 0.399, Val loss 0.701\n",
      "Ep 3 (Step 002135): Train loss 0.368, Val loss 0.694\n",
      "Ep 3 (Step 002140): Train loss 0.429, Val loss 0.693\n",
      "Ep 3 (Step 002145): Train loss 0.397, Val loss 0.690\n",
      "Ep 3 (Step 002150): Train loss 0.327, Val loss 0.685\n",
      "Ep 3 (Step 002155): Train loss 0.457, Val loss 0.689\n",
      "Ep 3 (Step 002160): Train loss 0.274, Val loss 0.694\n",
      "Ep 3 (Step 002165): Train loss 0.362, Val loss 0.692\n",
      "Ep 3 (Step 002170): Train loss 0.275, Val loss 0.692\n",
      "Ep 3 (Step 002175): Train loss 0.445, Val loss 0.686\n",
      "Ep 3 (Step 002180): Train loss 0.372, Val loss 0.679\n",
      "Ep 3 (Step 002185): Train loss 0.344, Val loss 0.676\n",
      "Ep 3 (Step 002190): Train loss 0.455, Val loss 0.678\n",
      "Ep 3 (Step 002195): Train loss 0.269, Val loss 0.680\n",
      "Ep 3 (Step 002200): Train loss 0.432, Val loss 0.685\n",
      "Ep 3 (Step 002205): Train loss 0.288, Val loss 0.694\n",
      "Ep 3 (Step 002210): Train loss 0.544, Val loss 0.701\n",
      "Ep 3 (Step 002215): Train loss 0.364, Val loss 0.702\n",
      "Ep 3 (Step 002220): Train loss 0.298, Val loss 0.689\n",
      "Ep 3 (Step 002225): Train loss 0.330, Val loss 0.684\n",
      "Ep 3 (Step 002230): Train loss 0.346, Val loss 0.689\n",
      "Ep 3 (Step 002235): Train loss 0.293, Val loss 0.693\n",
      "Ep 3 (Step 002240): Train loss 0.244, Val loss 0.697\n",
      "Ep 3 (Step 002245): Train loss 0.424, Val loss 0.700\n",
      "Ep 3 (Step 002250): Train loss 0.259, Val loss 0.712\n",
      "Ep 3 (Step 002255): Train loss 0.301, Val loss 0.723\n",
      "Ep 3 (Step 002260): Train loss 0.292, Val loss 0.730\n",
      "Ep 3 (Step 002265): Train loss 0.292, Val loss 0.733\n",
      "Ep 3 (Step 002270): Train loss 0.271, Val loss 0.736\n",
      "Ep 3 (Step 002275): Train loss 0.400, Val loss 0.735\n",
      "Ep 3 (Step 002280): Train loss 0.380, Val loss 0.736\n",
      "Ep 3 (Step 002285): Train loss 0.195, Val loss 0.734\n",
      "Ep 3 (Step 002290): Train loss 0.326, Val loss 0.731\n",
      "Ep 3 (Step 002295): Train loss 0.347, Val loss 0.731\n",
      "Ep 3 (Step 002300): Train loss 0.253, Val loss 0.738\n",
      "Ep 3 (Step 002305): Train loss 0.264, Val loss 0.747\n",
      "Ep 3 (Step 002310): Train loss 0.429, Val loss 0.750\n",
      "Ep 3 (Step 002315): Train loss 0.289, Val loss 0.755\n",
      "Ep 3 (Step 002320): Train loss 0.376, Val loss 0.764\n",
      "Ep 3 (Step 002325): Train loss 0.325, Val loss 0.770\n",
      "Ep 3 (Step 002330): Train loss 0.352, Val loss 0.777\n",
      "Ep 3 (Step 002335): Train loss 0.339, Val loss 0.774\n",
      "Ep 3 (Step 002340): Train loss 0.270, Val loss 0.778\n",
      "Ep 3 (Step 002345): Train loss 0.240, Val loss 0.786\n",
      "Ep 3 (Step 002350): Train loss 0.315, Val loss 0.790\n",
      "Ep 3 (Step 002355): Train loss 0.209, Val loss 0.796\n",
      "Ep 3 (Step 002360): Train loss 0.325, Val loss 0.790\n",
      "Ep 3 (Step 002365): Train loss 0.330, Val loss 0.776\n",
      "Ep 3 (Step 002370): Train loss 0.238, Val loss 0.756\n",
      "Ep 3 (Step 002375): Train loss 0.297, Val loss 0.749\n",
      "Ep 3 (Step 002380): Train loss 0.341, Val loss 0.734\n",
      "Ep 3 (Step 002385): Train loss 0.281, Val loss 0.725\n",
      "Ep 3 (Step 002390): Train loss 0.343, Val loss 0.723\n",
      "Ep 3 (Step 002395): Train loss 0.293, Val loss 0.719\n",
      "Ep 3 (Step 002400): Train loss 0.262, Val loss 0.718\n",
      "Ep 3 (Step 002405): Train loss 0.250, Val loss 0.712\n",
      "Ep 3 (Step 002410): Train loss 0.350, Val loss 0.712\n",
      "Ep 3 (Step 002415): Train loss 0.354, Val loss 0.719\n",
      "Ep 3 (Step 002420): Train loss 0.229, Val loss 0.732\n",
      "Ep 3 (Step 002425): Train loss 0.306, Val loss 0.743\n",
      "Ep 3 (Step 002430): Train loss 0.346, Val loss 0.744\n",
      "Ep 3 (Step 002435): Train loss 0.351, Val loss 0.742\n",
      "Ep 3 (Step 002440): Train loss 0.194, Val loss 0.734\n",
      "Ep 3 (Step 002445): Train loss 0.246, Val loss 0.734\n",
      "Ep 3 (Step 002450): Train loss 0.311, Val loss 0.747\n",
      "Ep 3 (Step 002455): Train loss 0.283, Val loss 0.748\n",
      "Ep 3 (Step 002460): Train loss 0.379, Val loss 0.742\n",
      "Ep 3 (Step 002465): Train loss 0.271, Val loss 0.741\n",
      "Ep 3 (Step 002470): Train loss 0.266, Val loss 0.745\n",
      "Ep 3 (Step 002475): Train loss 0.193, Val loss 0.749\n",
      "Ep 3 (Step 002480): Train loss 0.242, Val loss 0.754\n",
      "Ep 3 (Step 002485): Train loss 0.332, Val loss 0.757\n",
      "Ep 3 (Step 002490): Train loss 0.312, Val loss 0.759\n",
      "Ep 3 (Step 002495): Train loss 0.299, Val loss 0.761\n",
      "Ep 3 (Step 002500): Train loss 0.254, Val loss 0.761\n",
      "Ep 3 (Step 002505): Train loss 0.286, Val loss 0.761\n",
      "Ep 3 (Step 002510): Train loss 0.316, Val loss 0.760\n",
      "Ep 3 (Step 002515): Train loss 0.228, Val loss 0.760\n",
      "Ep 3 (Step 002520): Train loss 0.324, Val loss 0.759\n",
      "Ep 3 (Step 002525): Train loss 0.347, Val loss 0.764\n",
      "Ep 3 (Step 002530): Train loss 0.304, Val loss 0.768\n",
      "Ep 3 (Step 002535): Train loss 0.221, Val loss 0.775\n",
      "Ep 3 (Step 002540): Train loss 0.286, Val loss 0.774\n",
      "Ep 3 (Step 002545): Train loss 0.271, Val loss 0.776\n",
      "Ep 3 (Step 002550): Train loss 0.277, Val loss 0.786\n",
      "Ep 3 (Step 002555): Train loss 0.321, Val loss 0.764\n",
      "Ep 3 (Step 002560): Train loss 0.290, Val loss 0.758\n",
      "Ep 3 (Step 002565): Train loss 0.283, Val loss 0.750\n",
      "Ep 3 (Step 002570): Train loss 0.321, Val loss 0.740\n",
      "Ep 3 (Step 002575): Train loss 0.316, Val loss 0.735\n",
      "Ep 3 (Step 002580): Train loss 0.306, Val loss 0.725\n",
      "Ep 3 (Step 002585): Train loss 0.271, Val loss 0.718\n",
      "Ep 3 (Step 002590): Train loss 0.305, Val loss 0.712\n",
      "Ep 3 (Step 002595): Train loss 0.322, Val loss 0.713\n",
      "Ep 3 (Step 002600): Train loss 0.409, Val loss 0.713\n",
      "Ep 3 (Step 002605): Train loss 0.233, Val loss 0.714\n",
      "Ep 3 (Step 002610): Train loss 0.282, Val loss 0.716\n",
      "Ep 3 (Step 002615): Train loss 0.299, Val loss 0.719\n",
      "Ep 3 (Step 002620): Train loss 0.307, Val loss 0.722\n",
      "Ep 3 (Step 002625): Train loss 0.320, Val loss 0.725\n",
      "Ep 3 (Step 002630): Train loss 0.243, Val loss 0.727\n",
      "Ep 3 (Step 002635): Train loss 0.245, Val loss 0.732\n",
      "Ep 3 (Step 002640): Train loss 0.249, Val loss 0.737\n",
      "Ep 3 (Step 002645): Train loss 0.215, Val loss 0.743\n",
      "Ep 3 (Step 002650): Train loss 0.225, Val loss 0.756\n",
      "Ep 3 (Step 002655): Train loss 0.216, Val loss 0.759\n",
      "Ep 3 (Step 002660): Train loss 0.267, Val loss 0.755\n",
      "Ep 3 (Step 002665): Train loss 0.226, Val loss 0.757\n",
      "Ep 3 (Step 002670): Train loss 0.250, Val loss 0.753\n",
      "Ep 3 (Step 002675): Train loss 0.266, Val loss 0.750\n",
      "Ep 3 (Step 002680): Train loss 0.220, Val loss 0.746\n",
      "Ep 3 (Step 002685): Train loss 0.250, Val loss 0.735\n",
      "Ep 3 (Step 002690): Train loss 0.265, Val loss 0.730\n",
      "Ep 3 (Step 002695): Train loss 0.260, Val loss 0.721\n",
      "Ep 3 (Step 002700): Train loss 0.311, Val loss 0.720\n",
      "Ep 3 (Step 002705): Train loss 0.258, Val loss 0.726\n",
      "Ep 3 (Step 002710): Train loss 0.364, Val loss 0.733\n",
      "Ep 3 (Step 002715): Train loss 0.242, Val loss 0.744\n",
      "Ep 3 (Step 002720): Train loss 0.238, Val loss 0.752\n",
      "Ep 3 (Step 002725): Train loss 0.230, Val loss 0.760\n",
      "Ep 3 (Step 002730): Train loss 0.322, Val loss 0.768\n",
      "Ep 3 (Step 002735): Train loss 0.294, Val loss 0.770\n",
      "Ep 3 (Step 002740): Train loss 0.251, Val loss 0.775\n",
      "Ep 3 (Step 002745): Train loss 0.274, Val loss 0.773\n",
      "Ep 3 (Step 002750): Train loss 0.313, Val loss 0.775\n",
      "Ep 3 (Step 002755): Train loss 0.322, Val loss 0.777\n",
      "Ep 3 (Step 002760): Train loss 0.398, Val loss 0.779\n",
      "Ep 3 (Step 002765): Train loss 0.339, Val loss 0.785\n",
      "Ep 3 (Step 002770): Train loss 0.318, Val loss 0.793\n",
      "Ep 3 (Step 002775): Train loss 0.202, Val loss 0.795\n",
      "Ep 3 (Step 002780): Train loss 0.210, Val loss 0.792\n",
      "Ep 3 (Step 002785): Train loss 0.245, Val loss 0.776\n",
      "Ep 3 (Step 002790): Train loss 0.263, Val loss 0.765\n",
      "Ep 3 (Step 002795): Train loss 0.402, Val loss 0.764\n",
      "Ep 3 (Step 002800): Train loss 0.208, Val loss 0.758\n",
      "Text Generation Sample\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooked the meal every day.<|endoftext|>The results of the state food exam are given instructions. Read the instructions closely and follow directions, keeping track of what is in\n",
      "Training completed in 17.01 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Training - Fine tuning\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(gpt_a.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "train_losses, val_losses, tokens_seen = GPT.train_model_simple(\n",
    "    gpt_a, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=GPTA.format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "GPT.plot_values(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a bullet.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulus.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is John Piper Jnr.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract the response from the model\n",
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = GPTA.format_input(entry)\n",
    "\n",
    "    token_ids = GPT.text_generation(\n",
    "        model=gpt_a,\n",
    "        idx=GPT.text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        num_token_generation=256,\n",
    "        context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = GPT.token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Load the model '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(gpt_a.state_dict(), \"Assistant.pth\")\n",
    "\n",
    "\"\"\" Load the model \"\"\"\n",
    "# model_state_dict = torch.load(\"Assistant.pth\", map_location=device, weights_only=True)\n",
    "# model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearninglab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
